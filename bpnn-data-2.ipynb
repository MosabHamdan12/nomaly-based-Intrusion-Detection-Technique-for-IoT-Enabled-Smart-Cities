{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install xlrd==1.2.0","metadata":{"execution":{"iopub.status.busy":"2022-04-26T10:09:17.500169Z","iopub.execute_input":"2022-04-26T10:09:17.500476Z","iopub.status.idle":"2022-04-26T10:09:26.856979Z","shell.execute_reply.started":"2022-04-26T10:09:17.500436Z","shell.execute_reply":"2022-04-26T10:09:26.855836Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"pip install xlsxwriter","metadata":{"execution":{"iopub.status.busy":"2022-04-26T10:09:26.859196Z","iopub.execute_input":"2022-04-26T10:09:26.859560Z","iopub.status.idle":"2022-04-26T10:09:36.818283Z","shell.execute_reply.started":"2022-04-26T10:09:26.859521Z","shell.execute_reply":"2022-04-26T10:09:36.817222Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xlrd\nfrom sklearn.preprocessing import  MinMaxScaler \nfrom sklearn.neural_network import  MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split as split\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix , precision_score, recall_score \nfrom sklearn.metrics import jaccard_score, f1_score\n#### import data\nworkbook = xlrd.open_workbook(\"../input/file-11/training set.xlsx\",\"rb\")\nsheets = workbook.sheet_names()\nsheet_num=1  ## add sheet number # 5\nsh = workbook.sheet_by_name(sheets[(sheet_num)-1])\nn_cols=0 ##number of colum you have to delet from the front\nn_rows=0 ##number of rows you have to delet from the front \nm_cols=0 ## number of colum you have to delet from the end\nm_rows=0 ## number of rows you have to delet from the end\nM_rows=sh.nrows-m_rows\nM_cols=sh.ncols-m_cols\nrequired_data = []\nfor rownum in range(n_rows,M_rows):\n    row_valaues = sh.row_values(rownum)\n    required_data.append(row_valaues[n_cols:M_cols])\nRequired_data=np.asarray(required_data)\nX=Required_data[1:82334,1:40].astype(float)  #  25194 # 82334  ( 4:41 & 1:40)\nX= np.array (X)\n\nY=Required_data[1:82334,41:42].astype(float).ravel()    # (42:43  & 41:42)\nprint(X.shape)\n\nbands = Required_data[0:1, 1:40]      # ( 4:41 & 1:40)\nbands= np.array (bands)\n\nX = MinMaxScaler().fit(X).transform(X)\n#===============\nX_train, X_test, Y_train, Y_test = split(X, Y, test_size=(0.20), random_state=0)\n\nlayer1_f = []\nlayer2_f = []\nactivation_f = []\nr2_train_f =[]\nr2_test_f = []\ncoefficient_features =[]\nfeatures_best = []\nloss_train_f=[]\nloss_test_f=[]\n\nindex = min(X_train.shape) \n\nfor j in range (0,index): # \n    n=min(X_train.shape) \n    layer11 = []\n    layer22 = []\n    activation1 = []\n    r2_train1 =[]\n    r2_test1 = []\n    loss_train1 = []\n    loss_test1 = []    \n    layer1 = 8\n    layer2 = 10\n    for activation in  ('identity','logistic', 'tanh', 'relu'): \n        \n        print (activation) \n\n        model = MLPClassifier(hidden_layer_sizes=(layer1,layer2),\n                                  activation=activation,\n                                  solver='lbfgs',\n                                  max_iter=300,\n                                  verbose=False, \n                                  alpha=1e-5, \n                                  tol=1e-5, \n                                  random_state=42)  \n            \n        BPNN_model=model.fit(X_train, Y_train)\n        #==========\n        Y_train_pred=BPNN_model.predict(X_train)  \n        Y_test_pred=BPNN_model.predict(X_test)\n        #===========\n        loss_train = log_loss(Y_train, Y_train_pred)\n        loss_test = log_loss(Y_test, Y_test_pred)\n        #===========\n        r2_train= accuracy_score(Y_train, Y_train_pred)\n        r2_test= accuracy_score(Y_test, Y_test_pred)\n        #==========\n        print (\"r2_train\",r2_train, 'loss_train', loss_train)\n        print (\"r2_test\",r2_test, 'loss_test', loss_test)                \n        layer11.append (layer1)\n        layer22.append (layer2) \n        activation1.append (activation)\n        r2_train1.append (r2_train)\n        r2_test1.append (r2_test)\n        loss_train1.append (loss_train)\n        loss_test1.append (loss_test)                \n        \n        print ('********************************')     \n    \n    loss_test_min = np.argmin (loss_test1)\n    layer1 = layer11[loss_test_min]\n    layer2 = layer22 [loss_test_min]\n    activation = activation1 [loss_test_min]                                \n    r2_train= r2_train1 [loss_test_min] \n    r2_test = r2_test1 [loss_test_min] \n    loss_train = loss_train1 [loss_test_min] \n    loss_test = loss_test1 [loss_test_min] \n    \n    layer1_f.append (layer1)\n    layer2_f.append (layer2)\n    activation_f.append (activation)\n    r2_train_f.append (r2_train)\n    r2_test_f.append (r2_test)\n    loss_train_f.append (loss_train)\n    loss_test_f.append (loss_test)\n    \n    model = MLPClassifier(hidden_layer_sizes=(layer1,layer2),\n                              activation=activation,\n                              solver='lbfgs',\n                              max_iter=300,\n                              verbose=False, \n                              alpha=1e-5, \n                              tol=1e-5, \n                              random_state=42)    \n    \n    BPNN_model=model.fit(X_train, Y_train)\n    \n             \n    #~~~~~~  select impotance features according to Eq. Glorfeld, 1996\n    W_1= MinMaxScaler().fit(abs(BPNN_model.coefs_[0]).T).transform(abs(BPNN_model.coefs_[0]).T) ## the weight between input layers and first hidden layer after normalization\n        \n    numerator=np.sum((abs(BPNN_model.coefs_[0])/np.sum(abs(BPNN_model.coefs_[0]), axis=0))*(np.sum(abs(BPNN_model.coefs_[1]), axis=1)), axis=1)\n\n    M=numerator/sum(numerator)    ## important features\n    Mm=MinMaxScaler().fit(M.reshape(-1, 1)).transform(M.reshape(-1, 1))\n\n    sorted_ind = np.argsort(np.abs(Mm[:,0])) # الاصغر فالاكبر\n    X_train = X_train[:,sorted_ind]\n    bands = bands [:,sorted_ind]\n    coefficient_features.append(Mm) \n    #~~~~~~~~~~~~~~~~~~\n    print (\"number of indices = \", min (X_train.shape) )\n    print ('The end-------------------------------------------')\n    features_best.append ([bands])\n    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    print ('The second loop')   \n    X_train = X_train[:, 1:]\n    X_test = X_test[:, 1:]\n    bands = bands [:, 1:]\n        \n#~~~ the higher R2_test\nloss_test_min_f = np.argmin (loss_test_f) \n#~~~~~~ best parameter\nlayer1_ = layer1_f [loss_test_min_f]\nlayer2_ = layer2_f [loss_test_min_f]\nactivation_ = activation_f [loss_test_min_f]\nr2_train_f_ =  r2_train_f [loss_test_min_f]\nr2_test_f_ = r2_test_f [loss_test_min_f]\nloss_test_f_ = loss_train_f [loss_test_min_f]\nloss_train_f_ = loss_test_f [loss_test_min_f]\n           \nfeatures_best_ = features_best [loss_test_min_f]\ncoefficient_features_ = coefficient_features [loss_test_min_f]\n\nprint (\"layer1 =\",layer1_)\nprint (\"layer2 =\",layer2_)\nprint ('activat=', activation_)\nprint ('r2_test=', r2_test_f_)\nprint ('r2_train=', r2_train_f_)\nprint ('loss_test=', loss_test_f_)\nprint ('loss_train=', loss_train_f_)\nprint (\"The best band =\",features_best_)\nprint ('====================')\n#~~~~~~~\nlayer1 = pd.DataFrame(layer1_f)\nlayer2 = pd.DataFrame(layer2_f)\nactivation = pd.DataFrame(activation_f)\nr2_train = pd.DataFrame (r2_train_f)\nr2_test = pd.DataFrame (r2_test_f)\nfeatures = pd.DataFrame (features_best)\ncoefficient = pd.DataFrame (coefficient_features)\nloss_train = pd.DataFrame (loss_train_f)\nloss_test = pd.DataFrame (loss_test_f)\n\nwriter = pd.ExcelWriter(\"BPNN_feature selection_data2\" + \".xlsx\", engine='xlsxwriter')\nlayer1.to_excel(writer, sheet_name='layer1', index=False, header=0)\nlayer2.to_excel(writer, sheet_name='layer2', index=False, header=0)\nactivation.to_excel(writer, sheet_name='activation', index=False, header=0)\nr2_train.to_excel(writer, sheet_name='r2_train', index=False, header=0)\nr2_test.to_excel(writer, sheet_name='r2_test', index=False, header=0)\nloss_train.to_excel(writer, sheet_name='loss_train', index=False, header=0)\nloss_test.to_excel(writer, sheet_name='loss_test', index=False, header=0)\nfeatures.to_excel(writer, sheet_name='features', index=False, header=0)\ncoefficient.to_excel(writer, sheet_name='coefficient', index=False, header=0)\nwriter.save() ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-26T10:09:36.820292Z","iopub.execute_input":"2022-04-26T10:09:36.820533Z","iopub.status.idle":"2022-04-26T11:17:30.844238Z","shell.execute_reply.started":"2022-04-26T10:09:36.820504Z","shell.execute_reply":"2022-04-26T11:17:30.843188Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xlrd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import  MinMaxScaler \nfrom sklearn.neural_network import  MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split as split\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix , precision_score, recall_score \nfrom sklearn.metrics import jaccard_score, f1_score\nimport time\nimport pickle\n\ntime1=time.time()\n\n#### import data\nworkbook = xlrd.open_workbook(\"../input/file-11/training set.xlsx\",\"rb\")\nsheets = workbook.sheet_names()\nsheet_num=1  ## add sheet number # 5\nsh = workbook.sheet_by_name(sheets[(sheet_num)-1])\nn_cols=0 ##number of colum you have to delet from the front\nn_rows=0 ##number of rows you have to delet from the front \nm_cols=0 ## number of colum you have to delet from the end\nm_rows=0 ## number of rows you have to delet from the end\nM_rows=sh.nrows-m_rows\nM_cols=sh.ncols-m_cols\nrequired_data = []\nfor rownum in range(n_rows,M_rows):\n    row_valaues = sh.row_values(rownum)\n    required_data.append(row_valaues[n_cols:M_cols])\nRequired_data=np.asarray(required_data)\nX=Required_data[1:82334,1:40].astype(float)  #  25194 # 82334  ( 4:41 & 1:40)\nX= np.array (X)\n\nY=Required_data[1:82334,41:42].astype(float).ravel()    # (42:43  & 41:42)\nprint(X.shape)\n\nbands = Required_data[0:1, 1:40]      # ( 4:41 & 1:40)\nbands= np.array (bands)\n\nX = MinMaxScaler().fit(X).transform(X)\n#===============\nX_train, X_test, Y_train, Y_test = split(X, Y, test_size=(0.20), random_state=0)\n\nlayer1 = 8     # please put the best selected number \nlayer2 =  10       # please put the best selected number \nactivation = 'tanh'       # please put the best selected function \n\n\nmodel = MLPClassifier(hidden_layer_sizes=(layer1,layer2),\n                          activation=activation,\n                          solver='lbfgs',\n                          max_iter=300,\n                          verbose=False, \n                          alpha=1e-5, \n                          tol=1e-5, \n                          random_state=42)    \n\nBPNN_model=model.fit(X_train, Y_train)\n\nprint ((time.time()-time1))\n\n# save the model to disk\nfilename = 'BPNN_data_2_model.h5'\npickle.dump(BPNN_model, open(filename, 'wb'))\n \n\n\nY_train_pred=BPNN_model.predict(X_train)  \nY_test_pred=BPNN_model.predict(X_test)\n#===========\nloss_train = log_loss(Y_train, Y_train_pred)\nloss_test = log_loss(Y_test, Y_test_pred)\n#===========\nr2_train= accuracy_score(Y_train, Y_train_pred)\nr2_test= accuracy_score(Y_test, Y_test_pred)\n#==========\nprint (\"r2_train\",r2_train, 'loss_train', loss_train)\nprint (\"r2_test\",r2_test, 'loss_test', loss_test)   \n\nCM = confusion_matrix(Y_test, Y_test_pred)\nTN = CM[0][0]\nFN = CM[1][0]\nTP = CM[1][1]\nFP = CM[0][1]\n#================ classification metrics\n# Sensitivity, hit rate, recall, or true positive rate\nTPR = TP/(TP+FN)\nprint ('TPR',TPR)   \n# Specificity or true negative rate\nTNR = TN/(TN+FP) \nprint('TNR',TNR)\n# Precision or positive predictive value\nPPV = TP/(TP+FP)\nprint ('PPV', PPV)\n# Negative predictive value\nNPV = TN/(TN+FN)\nprint('NPV', NPV)\n# Fall out or false positive rate\nFPR = FP/(FP+TN)\nprint('FPR',FPR)\n# False negative rate\nFNR = FN/(TP+FN)\nprint ('FNR',FNR)  \n# False discovery rate\nFDR = FP/(TP+FP)\nprint ('FDR',FDR)    \n# Overall accuracy   \nACC = (TP+TN)/(TP+FP+FN+TN)\nprint ('ACC',ACC)\n\nprecision = precision_score(Y_test, Y_test_pred, average='weighted')\nprint('Precision: %f' % precision)\n# recall: tp / (tp + fn)\nrecall = recall_score(Y_test, Y_test_pred, average='weighted')\nprint('Recall: %f' % recall)\n# f1: tp / (tp + fp + fn)\nf1 = f1_score(Y_test, Y_test_pred, average='weighted')\nprint('F1 score: %f' % f1)\n#-----------  IoU\nprint ('IoU:', jaccard_score(Y_test, Y_test_pred, average='micro'))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:29:24.307280Z","iopub.execute_input":"2022-04-26T11:29:24.307586Z","iopub.status.idle":"2022-04-26T11:31:03.901569Z","shell.execute_reply.started":"2022-04-26T11:29:24.307540Z","shell.execute_reply":"2022-04-26T11:31:03.900480Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\nX_train, X_test, Y_train, Y_test=split(X, Y, test_size=(0.20), random_state=0)\n\nRMSE_test = []\nr2_val11 = []\nr2_train =[]\nr2_test = []\nDifference = []\nmse_cv = []\nY_test_val_cv = []\nY_test_val_pred_cv = []\nrpd_cv1 = []\nRMSEC_train = []\nr2_val =[]\n#~~~~~~~~~~~~~\nYprdict_test = []\nYtrue_test = []\n#~~~~~~~~~~~~~\nN_EPOCHS = 301\nepoch = 1\n#~~~~~~~~~~~~\nlayer1 = 8    \nlayer2= 12\nactiva = 'tanh'   # identity','logistic', 'tanh', 'relu'\n\nr2_train1 =[]\nr2_test1 = []\nloss_train1 = []\nloss_test1 = [] \n    \nwhile epoch < N_EPOCHS:\n    print('epoch: ', epoch)\n    \n    model = MLPClassifier(hidden_layer_sizes=(layer1,layer2),\n                              activation=activation,\n                              solver='lbfgs',\n                              max_iter=epoch,\n                              verbose=False, \n                              alpha=1e-5, \n                              tol=1e-5, \n                              random_state=42)    \n    \n    BPNN_model=model.fit(X_train, Y_train)\n    \n    Y_train_pred=BPNN_model.predict(X_train)  \n    Y_test_pred=BPNN_model.predict(X_test)\n    #==========\n    print ('epoch', epoch)\n    print (\"r2_train\",r2_train, 'loss_train', loss_train)\n    print (\"r2_test\",r2_test, 'loss_test', loss_test) \n    #===========\n    loss_train = log_loss(Y_train, Y_train_pred)\n    loss_test = log_loss(Y_test, Y_test_pred)\n    loss_train1.append (loss_train)\n    loss_test1.append (loss_test)\n    #===========\n    r2_train= accuracy_score(Y_train, Y_train_pred)\n    r2_test= accuracy_score(Y_test, Y_test_pred)\n    r2_train1.append (r2_train)\n    r2_test1.append (r2_test)\n    epoch += 1\n    \n\nimport pandas as pd\nr2_train1= pd.DataFrame(data= r2_train1) \nr2_test1= pd.DataFrame(data= r2_test1) \n\nloss_train1= pd.DataFrame(data= loss_train1) \nloss_test1 = pd.DataFrame(data= loss_test1)\n\n\"\"\" save file to plot the results manually \"\"\"\n\nwriter = pd.ExcelWriter(\"Epoch_data_2\"+ \".xlsx\", engine='xlsxwriter')\nr2_train1.to_excel(writer, sheet_name='r2_train1', index=False, header=0)# all R2 in the sheet data\nr2_test1.to_excel(writer, sheet_name='r2_test1', index=False, header=0)# all R2 in the sheet data\nloss_train1.to_excel(writer, sheet_name='loss_train1', index=False, header=0)# all R2 in the sheet data\nloss_test1.to_excel(writer, sheet_name='loss_test1', index=False, header=0)# all R2 in the sheet data\nwriter.save()\n\n\"\"\" Plot \"\"\"\n\nepochs=range(1, len(r2_train1)+1)\n\nfont = {'family' : 'serif',\n        'color'  : 'black',\n        'weight' : 'normal',\n        'size'   : 12}\n\nloss_train1 = np.array (loss_train1)\nloss_test1 = np.array (loss_test1)\n\nplt.plot(epochs, loss_train1, 'r', label='Training')\nplt.plot(epochs, loss_test1, 'g', label='Validation')\nplt.title('Training and Validation Loss', fontdict=font)\nplt.xlabel('Epochs', fontdict=font)\nplt.ylabel('Loss', fontdict=font)\nplt.legend()\nplt.savefig(\"loss.png\")\nplt.show()\n#==========\n\nr2_train1 = np.array (r2_train1)\nr2_test1 = np.array (r2_test1)\n\nplt.plot(epochs, r2_train1, 'r', label='Training')\nplt.plot(epochs, r2_test1, 'g', label='Validation')\nplt.title('Training and Validation Score', fontdict=font)\nplt.xlabel('Epochs', fontdict=font)\nplt.ylabel('Accuracy', fontdict=font)\nplt.legend()\nplt.savefig(\"accuracy.png\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:38:29.547922Z","iopub.execute_input":"2022-04-26T11:38:29.548234Z","iopub.status.idle":"2022-04-26T13:20:50.481861Z","shell.execute_reply.started":"2022-04-26T11:38:29.548203Z","shell.execute_reply":"2022-04-26T13:20:50.480789Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class_names = ['normal','Attack']\n\n\ndef plot_confusion_matrix(y_true, y_pred, classes,normalize=False,title=None, cmap=plt.cm.Reds):\n                                              \n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = None    ### 'Normalized confusion matrix'\n        else:\n            title = None        ### 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(Y_test, Y_test_pred)\n    # Only use the labels that appear in the data\n#    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    fig, ax = plt.subplots(figsize=(8,8))\n    ax.tick_params(labelsize=12)       #777777777777\n    cmap=plt.cm.Reds                      #  cmap=plt.cm.Blues\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    cbar = ax.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n    for t in cbar.ax.get_yticklabels():\n        t.set_fontsize(12)\n    \n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",rotation_mode=\"anchor\")\n             \n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\nfont = {'family' : 'Times New Roman',\n        'color'  : 'black',\n        'weight' : 'bold',\n        'size'   : 18}\n\nnp.set_printoptions(precision=2) ########################## 2\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(Y_test, Y_test_pred, classes=class_names,title= None)             \nplt.xlabel('Predicted label', fontdict=font)\nplt.ylabel('True label',fontdict=font)\nplt.savefig('confusion matrix_1_.png')\nplt.show()\n\n# Plot normalized confusion matrix\nplot_confusion_matrix(Y_test, Y_test_pred, classes=class_names, normalize=True,title=None)            \nplt.xlabel('Predicted label', fontdict=font)\nplt.ylabel('True label',  fontdict=font)\nplt.savefig('confusion matrix_2_.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-26T13:32:38.236114Z","iopub.execute_input":"2022-04-26T13:32:38.236810Z","iopub.status.idle":"2022-04-26T13:32:39.378232Z","shell.execute_reply.started":"2022-04-26T13:32:38.236761Z","shell.execute_reply":"2022-04-26T13:32:39.377053Z"},"trusted":true},"execution_count":21,"outputs":[]}]}