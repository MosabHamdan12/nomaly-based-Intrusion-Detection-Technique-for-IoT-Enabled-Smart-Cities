{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install xlrd==1.2.0 ### ######","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-26T14:55:11.200645Z","iopub.execute_input":"2022-04-26T14:55:11.201212Z","iopub.status.idle":"2022-04-26T14:55:18.892204Z","shell.execute_reply.started":"2022-04-26T14:55:11.201172Z","shell.execute_reply":"2022-04-26T14:55:18.891300Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"pip install xlsxwriter","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:55:18.894823Z","iopub.execute_input":"2022-04-26T14:55:18.895114Z","iopub.status.idle":"2022-04-26T14:55:26.218908Z","shell.execute_reply.started":"2022-04-26T14:55:18.895078Z","shell.execute_reply":"2022-04-26T14:55:26.218035Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"name_file = \"LSTM_dataset_2\"","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:55:26.223115Z","iopub.execute_input":"2022-04-26T14:55:26.223411Z","iopub.status.idle":"2022-04-26T14:55:26.230366Z","shell.execute_reply.started":"2022-04-26T14:55:26.223375Z","shell.execute_reply":"2022-04-26T14:55:26.229688Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics , cross_decomposition, model_selection\nfrom sklearn.model_selection import cross_validate\nfrom scipy.signal import savgol_filter\nimport pandas as pd\nimport numpy as np\nimport xlrd\nimport xlsxwriter\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler #for calculate SNV\nimport time\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler ## for calculate SNV\nfrom sklearn.model_selection import train_test_split as split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import LeaveOneOut \nfrom sys import stdout\nimport re\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom sklearn.model_selection import train_test_split   \nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom keras.layers import Input\nfrom keras.utils import np_utils\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ write name of output file name \ntime1=time.time()\n\n# ================= import data\nworkbook = xlrd.open_workbook(\"../input/data-2/training set.xlsx\",\"rb\")\nsheets = workbook.sheet_names()\nsheet_num=1  ## add sheet number # 5\nsh = workbook.sheet_by_name(sheets[(sheet_num)-1])\nn_cols=0 ##number of colum you have to delet from the front\nn_rows=0 ##number of rows you have to delet from the front \nm_cols=0 ## number of colum you have to delet from the end\nm_rows=0 ## number of rows you have to delet from the end\nM_rows=sh.nrows-m_rows\nM_cols=sh.ncols-m_cols\nrequired_data = []\nfor rownum in range(n_rows,M_rows):\n    row_valaues = sh.row_values(rownum)\n    required_data.append(row_valaues[n_cols:M_cols])\nRequired_data=np.asarray(required_data)\nX=Required_data[1:82334,1:40].astype(float)  #  25194 # 82334  ( 4:41 & 1:40)\nX= np.array (X)\n\nY=Required_data[1:82334,41:42].astype(float).ravel()    # (42:43  & 41:42)\nprint(X.shape)\n\nbands = Required_data[0:1, 1:40]      # ( 4:41 & 1:40)\nbands= np.array (bands)\n#===============\nX_train, X_test, y_train, y_test = split(X, Y, test_size=(0.20), random_state=0)\n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\n# ==============================================\n\nnb_classes = len (np.unique( y_train ))\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\nX_train = X_train.reshape(X_train.shape[0], 39, 1)   #Reshape for lstm - should work!!\nX_test= X_test.reshape(X_test.shape[0], 39, 1)\n\nprint('class', nb_classes)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:55:26.234190Z","iopub.execute_input":"2022-04-26T14:55:26.235537Z","iopub.status.idle":"2022-04-26T14:56:17.214159Z","shell.execute_reply.started":"2022-04-26T14:55:26.235501Z","shell.execute_reply":"2022-04-26T14:56:17.213392Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Conv2D, Flatten, MaxPooling2D,BatchNormalization\nfrom keras.layers import LSTM,TimeDistributed,Conv1D,MaxPooling1D\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras import models\nfrom keras import layers\nfrom keras.models import Model\n\nmodel_lstm = Sequential() #=============================== climate-based model\nmodel_lstm.add(LSTM(units = 512, return_sequences = True, input_shape = (39, 1), activation='tanh'))\nmodel_lstm.add(LSTM(units = 256, return_sequences = True))\nmodel_lstm.add(Dense(128, activation='relu'))\nmodel_lstm.add(Dense(64, activation='relu'))\nmodel_lstm.add(Dropout(0.2))\nmodel_lstm.add(Dense(32, activation='relu'))\nmodel_lstm.add(Dropout(0.2))\nmodel_lstm.add(Flatten())\nmodel_lstm.add(Dense(nb_classes, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:56:17.215318Z","iopub.execute_input":"2022-04-26T14:56:17.215912Z","iopub.status.idle":"2022-04-26T14:56:17.844822Z","shell.execute_reply.started":"2022-04-26T14:56:17.215873Z","shell.execute_reply":"2022-04-26T14:56:17.844098Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(model_lstm, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:56:17.846129Z","iopub.execute_input":"2022-04-26T14:56:17.846369Z","iopub.status.idle":"2022-04-26T14:56:18.040096Z","shell.execute_reply.started":"2022-04-26T14:56:17.846327Z","shell.execute_reply":"2022-04-26T14:56:18.039354Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import CSVLogger\n\nmodel_lstm.compile(loss=keras.losses.binary_crossentropy,optimizer='RMSprop',metrics=['accuracy'])  \n\ncsv_logger = CSVLogger('training.log', separator=',', append=False)\nmc = ModelCheckpoint(name_file +'.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\ncnn11 = model_lstm.fit(X_train,Y_train, batch_size=300, epochs=500, validation_data=(X_test ,Y_test), callbacks=[mc,csv_logger])\nprint ((\"Training time=\", time.time()-time1))\nnp.save(\"LSTM_data_2_@history\", cnn11.history)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:56:18.042011Z","iopub.execute_input":"2022-04-26T14:56:18.042477Z","iopub.status.idle":"2022-04-26T14:56:44.896210Z","shell.execute_reply.started":"2022-04-26T14:56:18.042441Z","shell.execute_reply":"2022-04-26T14:56:44.894419Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"###################### 2\nfrom contextlib import redirect_stdout\nwith open('./'+name_file+\".xls\", 'w') as f:\n    with redirect_stdout(f):\n        model_lstm.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:56:51.028021Z","iopub.execute_input":"2022-04-26T14:56:51.028287Z","iopub.status.idle":"2022-04-26T14:56:51.036409Z","shell.execute_reply.started":"2022-04-26T14:56:51.028258Z","shell.execute_reply":"2022-04-26T14:56:51.035281Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nlog_data = pd.read_csv('./training.log', sep=',', engine='python')\n#print (log_data)\nLSTM =log_data","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:56:53.616990Z","iopub.execute_input":"2022-04-26T14:56:53.617267Z","iopub.status.idle":"2022-04-26T14:56:53.625589Z","shell.execute_reply.started":"2022-04-26T14:56:53.617238Z","shell.execute_reply":"2022-04-26T14:56:53.624832Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"print (name_file)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:56:56.037116Z","iopub.execute_input":"2022-04-26T14:56:56.037377Z","iopub.status.idle":"2022-04-26T14:56:56.042235Z","shell.execute_reply.started":"2022-04-26T14:56:56.037347Z","shell.execute_reply":"2022-04-26T14:56:56.041477Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nLSTM11_history_11=np.load('./LSTM_data_2_@history.npy',allow_pickle='TRUE').item()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:56:58.036876Z","iopub.execute_input":"2022-04-26T14:56:58.037673Z","iopub.status.idle":"2022-04-26T14:56:58.042925Z","shell.execute_reply.started":"2022-04-26T14:56:58.037628Z","shell.execute_reply":"2022-04-26T14:56:58.042228Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\n####################### 1\nname_file11 = './LSTM_dataset_2.h5' \nfashion_model = load_model(name_file11) # load model\nfashion_model.summary() # summarize model.","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:57:00.602247Z","iopub.execute_input":"2022-04-26T14:57:00.602486Z","iopub.status.idle":"2022-04-26T14:57:01.260140Z","shell.execute_reply.started":"2022-04-26T14:57:00.602448Z","shell.execute_reply":"2022-04-26T14:57:01.259228Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"val_loss, val_accuracy=fashion_model.evaluate(X_test ,Y_test) ## to get test accuracy and losses\nprint(val_loss, val_accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:57:04.388238Z","iopub.execute_input":"2022-04-26T14:57:04.388522Z","iopub.status.idle":"2022-04-26T14:57:08.311042Z","shell.execute_reply.started":"2022-04-26T14:57:04.388490Z","shell.execute_reply":"2022-04-26T14:57:08.310288Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"\ntime2=time.time()\npredict_prob=fashion_model.predict(X_test)\ny_pred=np.argmax(predict_prob,axis=1)\nprint ('classification time:', time.time()-time2)\n\n##print (y_pred)\ny_true=np.argmax(Y_test, axis=1)\nfrom sklearn.metrics import precision_recall_fscore_support as score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nCM = confusion_matrix(y_true, y_pred)\nprint (CM)\nprint(classification_report(y_true, y_pred))\n\nprecision = precision_score(y_test, y_pred, average='weighted')\nprint('Precision: %f' % precision)\n# recall: tp / (tp + fn)\nrecall = recall_score(y_test, y_pred, average='weighted')\nprint('Recall: %f' % recall)\n# f1: tp / (tp + fp + fn)\nf1 = f1_score(y_test, y_pred, average='weighted')\nprint('F1 score: %f' % f1)\n#-----------  IoU\nfrom sklearn.metrics import jaccard_score\nprint ('IoU:', jaccard_score(y_true, y_pred, average='micro'))\n\n\ntest_eval = fashion_model.evaluate(X_test, Y_test)\n\nloss, accuracy = fashion_model.evaluate(X_train, Y_train)\nprint('loss_train: ', loss, 'accuracy_train: ', accuracy)\nprint('Test loss:', test_eval[0], 'Test accuracy:', test_eval[1])\n\nTN = CM[0][0]\nFN = CM[1][0]\nTP = CM[1][1]\nFP = CM[0][1]\n#================ classification metrics\n# Sensitivity, hit rate, recall, or true positive rate\nTPR = TP/(TP+FN)\nprint ('TPR',TPR)   \n# Specificity or true negative rate\nTNR = TN/(TN+FP) \nprint('TNR',TNR)\n# Precision or positive predictive value\nPPV = TP/(TP+FP)\nprint ('PPV', PPV)\n# Negative predictive value\nNPV = TN/(TN+FN)\nprint('NPV', NPV)\n# Fall out or false positive rate\nFPR = FP/(FP+TN)\nprint('FPR',FPR)\n# False negative rate\nFNR = FN/(TP+FN)\nprint ('FNR',FNR)  \n# False discovery rate\nFDR = FP/(TP+FP)\nprint ('FDR',FDR)    \n# Overall accuracy   \nACC = (TP+TN)/(TP+FP+FN+TN)\nprint ('ACC',ACC)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:57:10.698418Z","iopub.execute_input":"2022-04-26T14:57:10.698866Z","iopub.status.idle":"2022-04-26T14:57:29.956183Z","shell.execute_reply.started":"2022-04-26T14:57:10.698826Z","shell.execute_reply":"2022-04-26T14:57:29.955500Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"history_dict = LSTM11_history_11 ","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:57:31.530432Z","iopub.execute_input":"2022-04-26T14:57:31.530995Z","iopub.status.idle":"2022-04-26T14:57:31.535364Z","shell.execute_reply.started":"2022-04-26T14:57:31.530953Z","shell.execute_reply":"2022-04-26T14:57:31.534666Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#history_dict=cnn11.history\nloss_values=history_dict['loss']\nval_loss_values=history_dict['val_loss']\nacc_values=history_dict['accuracy']\nval_acc_values=history_dict['val_accuracy']\nepochs=range(1, len(acc_values)+1)\ndef smooth_curve(points, factor=0.8):\n    smoothed_points = []\n    for point in points:\n        if smoothed_points:\n            previous = smoothed_points[-1]\n            smoothed_points.append(previous * factor + point * (1 - factor))\n        else:\n            smoothed_points.append(point)\n    return smoothed_points\nloss_values=smooth_curve(loss_values)\nval_loss_values=smooth_curve(val_loss_values)\nacc_values=smooth_curve(acc_values)\nval_acc_values=smooth_curve(val_acc_values)\n\nfont = {'family' : 'serif',\n        'color'  : 'black',\n        'weight' : 'normal',\n        'size'   : 12}\n        \n\nplt.plot(epochs, acc_values, 'ro', label='Training acc')\nplt.plot(epochs, val_acc_values, 'g', label='Validation acc')\nplt.title('Training and Validation acc', fontdict=font)\nplt.xlabel('Epochs', fontdict=font)\nplt.ylabel('Accuracy', fontdict=font)\nplt.legend()\nplt.savefig(\"accuracy\"+name_file+\".png\")\nplt.show()\n\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'r', label='Validation loss')\nplt.title('Training and Validation loss', fontdict=font)\nplt.xlabel('Epochs',fontdict=font)\nplt.ylabel('Loss',fontdict=font)\nplt.legend()\nplt.savefig(\"loss\"+name_file+\".png\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:57:34.793670Z","iopub.execute_input":"2022-04-26T14:57:34.793963Z","iopub.status.idle":"2022-04-26T14:57:35.316115Z","shell.execute_reply.started":"2022-04-26T14:57:34.793930Z","shell.execute_reply":"2022-04-26T14:57:35.315404Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"class_names = ['Normal','Attack']\n\n\ndef plot_confusion_matrix(y_true, y_pred, classes,normalize=False,title=None, cmap=plt.cm.Reds):\n                                              \n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = None    ### 'Normalized confusion matrix'\n        else:\n            title = None        ### 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n#    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    fig, ax = plt.subplots(figsize=(8,8))\n    ax.tick_params(labelsize=12)       #7777777777777777777777777777777777777777777777777777\n    cmap=plt.cm.Reds                      #  cmap=plt.cm.Blues\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    cbar = ax.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n    for t in cbar.ax.get_yticklabels():\n        t.set_fontsize(15)\n    \n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",rotation_mode=\"anchor\")\n             \n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\nfont = {'family' : 'Times New Roman',\n        'color'  : 'black',\n        'weight' : 'bold',\n        'size'   : 15}\n\nnp.set_printoptions(precision=2) ########################## 2\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_test, y_pred, classes=class_names,title= None)             \nplt.xlabel('Predicted label', fontsize=18, font = 'Times New Roman')\nplt.ylabel('True label', fontsize=18, font = 'Times New Roman')\nplt.savefig('confusion matrix_1_'+name_file+'.png')\nplt.show()\n\n# Plot normalized confusion matrix\nplot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,title=None)            \nplt.xlabel('Predicted label', fontsize=18, font = 'Times New Roman')\nplt.ylabel('True label', fontsize=18, font = 'Times New Roman')\nplt.savefig('confusion matrix_2_'+name_file+'.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:57:39.914968Z","iopub.execute_input":"2022-04-26T14:57:39.915222Z","iopub.status.idle":"2022-04-26T14:57:40.633062Z","shell.execute_reply.started":"2022-04-26T14:57:39.915193Z","shell.execute_reply":"2022-04-26T14:57:40.632398Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'./LSTM_dataset_2.h5') #=======================","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:56:44.919211Z","iopub.status.idle":"2022-04-26T14:56:44.919921Z","shell.execute_reply.started":"2022-04-26T14:56:44.919644Z","shell.execute_reply":"2022-04-26T14:56:44.919670Z"},"trusted":true},"execution_count":null,"outputs":[]}]}